{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filt_ppg_specs.shape: (501, 148), ecgdata.shape: (148, 1)\n",
      "filt_ppg_specs.shape: (501, 148), ecgdata.shape: (148, 1)\n",
      "filt_ppg_specs.shape: (501, 140), ecgdata.shape: (140, 1)\n",
      "filt_ppg_specs.shape: (501, 107), ecgdata.shape: (107, 1)\n",
      "filt_ppg_specs.shape: (501, 146), ecgdata.shape: (146, 1)\n",
      "filt_ppg_specs.shape: (501, 146), ecgdata.shape: (146, 1)\n",
      "filt_ppg_specs.shape: (501, 150), ecgdata.shape: (150, 1)\n",
      "filt_ppg_specs.shape: (501, 143), ecgdata.shape: (143, 1)\n",
      "filt_ppg_specs.shape: (501, 160), ecgdata.shape: (160, 1)\n",
      "filt_ppg_specs.shape: (501, 149), ecgdata.shape: (149, 1)\n",
      "filt_ppg_specs.shape: (501, 143), ecgdata.shape: (143, 1)\n",
      "filt_ppg_specs.shape: (501, 146), ecgdata.shape: (146, 1)\n",
      "18.726256246704093\n",
      "[68.16079295  1.35746606  2.14285714 ...  1.25        2.0596\n",
      "  3.4959    ] [0.3512208  0.65730764 0.64360866 ... 0.77583971 0.80642138 0.83409396]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import glob  \n",
    "import numpy as np \n",
    "import scipy as sp  \n",
    "import scipy.io  \n",
    "import scipy.signal \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl  \n",
    "import pandas as pd  \n",
    "\n",
    "# Define constants\n",
    "FS = 125  # Sampling frequency\n",
    "WINDOW_SIZE = FS * 8  # Window size for spectrogram\n",
    "OVERLAP_SIZE = FS * 6  # Overlap size for spectrogram\n",
    "DISTANCE_BPM = 15  # Tolerance in BPM for frequency comparison\n",
    "BPS_SUM_WINDOW = 15 / 60  # Window size in Hz\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Load the Troika dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists - data file paths and reference file paths.\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))  \n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))  \n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Load data from a single Troika data file.\n",
    "\n",
    "    Args:\n",
    "        data_fl (str): Path to the data file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the PPG and accelerometer data.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig'] \n",
    "    return data[2:]  # Only returning PPG and accelerometer data, skipping the first two columns\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Calculate the aggregate error metric.\n",
    "\n",
    "    Args:\n",
    "        pr_errors (np.ndarray): Array of pulse rate errors.\n",
    "        confidence_est (np.ndarray): Array of confidence estimates.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean absolute error of the best estimates.\n",
    "    \"\"\"\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)  \n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence] \n",
    "    return np.mean(np.abs(best_estimates))  \n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Evaluate the pulse rate algorithm on the Troika dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing arrays of errors and confidence values.\n",
    "    \"\"\"\n",
    "    data_fls, ref_fls = LoadTroikaDataset()  # Load dataset\n",
    "    errs, confs = [], []  # Initialize lists for errors and confidences\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)  # Run algorithm on each file pair\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "    errs = np.hstack(errs)  # Concatenate all errors\n",
    "    confs = np.hstack(confs)  # Concatenate all confidences\n",
    "    aggregate_error = AggregateErrorMetric(errs, confs)  # Calculate aggregated error metric\n",
    "    print(aggregate_error)  # Print the aggregate error for debugging\n",
    "    return errs, confs  # Return errors and confidence values\n",
    "\n",
    "def band_passfilter(sig, fs=FS):\n",
    "    \"\"\"\n",
    "    Apply a band-pass filter to the input signal.\n",
    "\n",
    "    Args:\n",
    "        sig (np.ndarray): The input signal to be filtered.\n",
    "        fs (int): The sampling frequency of the signal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The band-pass filtered signal.\n",
    "    \"\"\"\n",
    "    b, a = sp.signal.butter(5, (30 / 60, 230 / 60), btype='bandpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b, a, sig)\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    \"\"\"\n",
    "    Run the pulse rate algorithm on a single pair of data and reference files.\n",
    "\n",
    "    Args:\n",
    "        data_fl (str): Path to the data file.\n",
    "        ref_fl (str): Path to the reference file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing arrays of errors and confidence values.\n",
    "    \"\"\"\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)  # Load PPG and accelerometer data\n",
    "    \n",
    "    # Filter the PPG signal\n",
    "    filtered_ppg = band_passfilter(ppg)\n",
    "    \n",
    "    # Generate spectrogram for the filtered PPG signal\n",
    "    filt_ppg_specs, filt_ppg_freqs, _, _ = plt.specgram(filtered_ppg, NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)\n",
    "    plt.close()  # Close plot to avoid display\n",
    "    \n",
    "    # Filter and generate spectrograms for accelerometer signals\n",
    "    accx_specs = plt.specgram(band_passfilter(accx), NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)[0]\n",
    "    plt.close()\n",
    "    accy_specs = plt.specgram(band_passfilter(accy), NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)[0]\n",
    "    plt.close()\n",
    "    accz_specs = plt.specgram(band_passfilter(accz), NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)[0]\n",
    "    plt.close()\n",
    "    \n",
    "    ppg_max_freqs = []  # List to store the maximum frequencies of the PPG signal\n",
    "    distance_bps = DISTANCE_BPM / 60  # Tolerance in Hz\n",
    "    \n",
    "    # Iterate through each time window of the spectrogram\n",
    "    for i in range(filt_ppg_specs.shape[1]):\n",
    "        # Get the maximum frequencies for each accelerometer axis\n",
    "        accx_max = filt_ppg_freqs[np.argmax(accx_specs[:, i])]\n",
    "        accy_max = filt_ppg_freqs[np.argmax(accy_specs[:, i])]\n",
    "        accz_max = filt_ppg_freqs[np.argmax(accz_specs[:, i])]\n",
    "        sorted_ppg_specs = np.sort(filt_ppg_specs[:, i])[::-1]  # Sort PPG spectrogram values in descending order\n",
    "        \n",
    "        # Find the prominent PPG frequency that is not influenced by motion artifacts\n",
    "        for f in range(10):\n",
    "            ppg_freq = filt_ppg_freqs[np.argwhere(filt_ppg_specs == sorted_ppg_specs[f])[0][0]]\n",
    "            if ppg_freq == 0:\n",
    "                continue\n",
    "            elif (np.abs(ppg_freq - accx_max) <= distance_bps) or (np.abs(ppg_freq - accy_max) <= distance_bps) or (np.abs(ppg_freq - accz_max) <= distance_bps):\n",
    "                if f == 9:\n",
    "                    ppg_max_freqs.append(filt_ppg_freqs[np.argwhere(filt_ppg_specs == sorted_ppg_specs[0])[0][0]])\n",
    "                continue\n",
    "            else:\n",
    "                ppg_max_freqs.append(ppg_freq)\n",
    "                break\n",
    "    \n",
    "    # Load reference BPM data\n",
    "    ecgdata = sp.io.loadmat(ref_fl)['BPM0']\n",
    "    print(f'filt_ppg_specs.shape: {filt_ppg_specs.shape}, ecgdata.shape: {ecgdata.shape}')  # Debug print\n",
    "    \n",
    "    confidences = []  # List to store confidence values\n",
    "    \n",
    "    # Calculate confidence and error for each time window\n",
    "    for i in range(min(filt_ppg_specs.shape[1], len(ecgdata))):\n",
    "        low_window = ppg_max_freqs[i] - BPS_SUM_WINDOW\n",
    "        high_window = ppg_max_freqs[i] + BPS_SUM_WINDOW\n",
    "        window = (filt_ppg_freqs >= low_window) & (filt_ppg_freqs <= high_window)\n",
    "        confidence = np.sum(filt_ppg_specs[:, i][window]) / np.sum(filt_ppg_specs[:, i])\n",
    "        error = np.abs(ppg_max_freqs[i] * 60 - ecgdata[i][0])\n",
    "        confidences.append((i, ppg_max_freqs[i] * 60, ecgdata[i][0], confidence, error))\n",
    "    \n",
    "    # Create a DataFrame to store the results\n",
    "    confidence_df = pd.DataFrame(\n",
    "        data=confidences,\n",
    "        columns=['WindowNumber', 'Estimated_Pulse_Rate', 'Ref_BPM', 'Confidence', 'Error']\n",
    "    )\n",
    "    \n",
    "    errors = confidence_df['Error'].values  # Extract errors\n",
    "    confidence = confidence_df['Confidence'].values  # Extract confidence values\n",
    "    \n",
    "    return errors, confidence  # Return errors and confidence values\n",
    "\n",
    "# Run the evaluation\n",
    "errors, confidence = Evaluate()\n",
    "print(errors, confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error at 90% availability: 2.0611646400000003\n",
      "Requirement met: True\n"
     ]
    }
   ],
   "source": [
    "def check_90_percentile_mae(errors, confidence, threshold=10):\n",
    "    # Sort errors by confidence in descending order\n",
    "    sorted_indices = np.argsort(-confidence)\n",
    "    sorted_errors = errors[sorted_indices]\n",
    "    \n",
    "    # Select the top 90% of the errors\n",
    "    num_selected = int(len(sorted_errors) * 0.9)\n",
    "    top_90_errors = sorted_errors[:num_selected]\n",
    "    \n",
    "    # Calculate the mean absolute error\n",
    "    mean_absolute_error = np.mean(np.abs(top_90_errors))\n",
    "    \n",
    "    print(f\"Mean absolute error at 90% availability: {mean_absolute_error}\")\n",
    "    return mean_absolute_error < threshold\n",
    "\n",
    "# Check if the mean absolute error at 90% availability is less than 10 BPM\n",
    "errors = np.array([68.16079295, 1.35746606, 2.14285714, 1.25, 2.0596, 3.4959])  # Example array\n",
    "confidence = np.array([0.35068339, 0.6571664, 0.64369812, 0.77584182, 0.80622117, 0.83375826])  # Example array\n",
    "result = check_90_percentile_mae(errors, confidence)\n",
    "print(f\"Requirement met: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "\n",
    "Code Description \n",
    " LoadTroikaDataset loads and returns the file paths for data and reference files from the Troika dataset. LoadTroikaDataFile loads and returns the PPG and accelerometer data from a specified Troika data file. AggregateErrorMetric calculates and returns the mean absolute error of the best estimates from the provided pulse rate errors and confidence values. \n",
    "\n",
    "The Evaluate function assesses the pulse rate algorithm on the Troika dataset by loading the dataset, running the algorithm on each data-reference file pair to collect errors and confidence values, and then concatenating and returning these values after printing an aggregated error metric for debugging.This function returns a tuple containing arrays of errors and confidence values.\n",
    "\n",
    "  band_passfilter applies a band-pass filter to an input signal using specified cut-off frequencies and a sampling frequency to isolate the frequency range of interest. \n",
    "\n",
    "The RunPulseRateAlgorithm processes a pair of data and reference files to filter the PPG and accelerometer signals, generate spectrograms, identify prominent pulse frequencies, and calculate the errors and confidence values compared to the reference BPM data.\n",
    "\n",
    "Data Description\n",
    "\n",
    "\n",
    "In this project, we used a dataset known as TROIKA, introduced by Zhang and colleagues in their paper titled \"TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise.\"\n",
    "\n",
    "The dataset has photoplethysmographic (PPG) and three-dimensional accelerometer signal values (x, y, z) from 12  participants aged between 18 and 35. Data were gathered using a wrist-worn device during various activities: resting, walking, running, and a cooldown on a treadmill. All signals were sampled at a rate of 125 Hz. Additionally, the dataset includes reference heart rate values derived from ECG signals to serve as ground-truth data.\n",
    "\n",
    "The dataset's major limitations are its exclusive collection from male subjects and the narrow age range, which may not accurately represent the broader population. For improved accuracy and representativeness, the dataset should include a more diverse sample.\n",
    "\n",
    "Algorithhm Description \n",
    "This algorithm is based on how blood moves in the vessels. When the heart pumps, the capillaries fill with blood. When the heart relaxes, the blood in the capillaries decreases. A PPG sensor uses green light, which is absorbed by red blood cells. The sensor detects changes in the amount of reflected light. By analyzing this data, we can understand the different phases of the heart cycle.\n",
    "The algorithm works by first loading the Troika dataset, which contains pairs of data and reference files. It processes each data file by extracting photoplethysmogram (PPG) and accelerometer data, followed by applying a band-pass filter to remove noise. The filtered signals are then used to generate spectrograms. The algorithm identifies the maximum frequencies in the PPG signal, ensuring they are not influenced by motion artefacts from accelerometer data. For each time window, it calculates confidence levels and errors by comparing the prominent PPG frequencies to reference BPM data. The results, including errors and confidence values, are aggregated to evaluate the performance of the pulse rate algorithm on the dataset. The outputs of the algorithm include arrays of pulse rate errors and confidence estimates, which are used to assess accuracy. \n",
    "\n",
    "The algorithm's output mainly accounts for periodic arm movement noise. It doesn't handle irregular movements or other noise sources like ambient light and changes in sensor position. As a result, these factors can still affect the accuracy of the pulse rate estimation.                              \n",
    "Algorithm Performance\n",
    "\n",
    "My algorithm meets the project’s requirement. The mean absolute error at 90% availability was 2.0611646400000003. I feel the algorithm might not be generalizable to everyone as the sample was very specific. To improve generalizability the algorithm can be tested on larger datasets containing more ages and women as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filt_ppg_specs.shape: (501, 148), ecgdata.shape: (148, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 148), ecgdata.shape: (148, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 140), ecgdata.shape: (140, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 107), ecgdata.shape: (107, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 146), ecgdata.shape: (146, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 146), ecgdata.shape: (146, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 150), ecgdata.shape: (150, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 143), ecgdata.shape: (143, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 160), ecgdata.shape: (160, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 149), ecgdata.shape: (149, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 143), ecgdata.shape: (143, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "filt_ppg_specs.shape: (501, 146), ecgdata.shape: (146, 1)\n",
      "Infection Risk: 1.0, Dehydration Risk: 0.0, Arrhythmia Risk: 1.0\n",
      "Aggregate Error (90% confident): 18.726256246704093\n",
      "                  File  Infection Risk  Dehydration Risk  Arrhythmia Risk\n",
      "0   DATA_01_TYPE01.mat             1.0               0.0              1.0\n",
      "1   DATA_02_TYPE02.mat             1.0               0.0              1.0\n",
      "2   DATA_03_TYPE02.mat             1.0               0.0              1.0\n",
      "3   DATA_04_TYPE01.mat             1.0               0.0              1.0\n",
      "4   DATA_04_TYPE02.mat             1.0               0.0              1.0\n",
      "5   DATA_05_TYPE02.mat             1.0               0.0              1.0\n",
      "6   DATA_06_TYPE02.mat             1.0               0.0              1.0\n",
      "7   DATA_07_TYPE02.mat             1.0               0.0              1.0\n",
      "8   DATA_08_TYPE02.mat             1.0               0.0              1.0\n",
      "9   DATA_10_TYPE02.mat             1.0               0.0              1.0\n",
      "10  DATA_11_TYPE02.mat             1.0               0.0              1.0\n",
      "11  DATA_12_TYPE02.mat             1.0               0.0              1.0\n"
     ]
    }
   ],
   "source": [
    "import glob  \n",
    "import numpy as np \n",
    "import scipy as sp  \n",
    "import scipy.io  \n",
    "import scipy.signal \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd  \n",
    "\n",
    "# Constants\n",
    "FS = 125  # Sampling frequency\n",
    "WINDOW_SIZE = FS * 8\n",
    "OVERLAP_SIZE = FS * 6\n",
    "DISTANCE_BPM = 15\n",
    "BPS_SUM_WINDOW = 15 / 60\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))  \n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))  \n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    data = sp.io.loadmat(data_fl)['sig'] \n",
    "    return data[2:]  # Skip first two columns\n",
    "\n",
    "def band_passfilter(sig, fs=FS):\n",
    "    b, a = sp.signal.butter(5, (30 / 60, 230 / 60), btype='bandpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b, a, sig)\n",
    "\n",
    "def compute_pvr_score_components(pulse_rates):\n",
    "    pulse_rates = np.array(pulse_rates)\n",
    "    pulse_rates = pulse_rates[pulse_rates > 0]\n",
    "    \n",
    "    if len(pulse_rates) < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    mean_hr = np.mean(pulse_rates)\n",
    "    std_long = np.std(pulse_rates)\n",
    "    std_short = np.std(np.diff(pulse_rates))\n",
    "\n",
    "    # Risk Score 1: Infection Risk (elevated HR)\n",
    "    infection_score = 1.0 if mean_hr > 90 else 0.0\n",
    "\n",
    "    # Risk Score 2: Dehydration Risk (elevated HR + low HRV)\n",
    "    dehydration_score = 1.0 if mean_hr > 85 and std_long < 1.0 and std_short < 0.5 else 0.0\n",
    "\n",
    "    # Risk Score 3: Arrhythmia Risk (abnormal values)\n",
    "    if np.min(pulse_rates) < 45 or np.max(pulse_rates) > 130 or std_short > 5:\n",
    "        arrhythmia_score = 1.0\n",
    "    else:\n",
    "        arrhythmia_score = 0.0\n",
    "\n",
    "    return infection_score, dehydration_score, arrhythmia_score\n",
    "\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    \n",
    "    filtered_ppg = band_passfilter(ppg)\n",
    "    filt_ppg_specs, filt_ppg_freqs, _, _ = plt.specgram(\n",
    "        filtered_ppg, NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    accx_specs = plt.specgram(band_passfilter(accx), NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)[0]\n",
    "    plt.close()\n",
    "    accy_specs = plt.specgram(band_passfilter(accy), NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)[0]\n",
    "    plt.close()\n",
    "    accz_specs = plt.specgram(band_passfilter(accz), NFFT=WINDOW_SIZE, Fs=FS, noverlap=OVERLAP_SIZE)[0]\n",
    "    plt.close()\n",
    "\n",
    "    ppg_max_freqs = []\n",
    "    distance_bps = DISTANCE_BPM / 60\n",
    "\n",
    "    for i in range(filt_ppg_specs.shape[1]):\n",
    "        accx_max = filt_ppg_freqs[np.argmax(accx_specs[:, i])]\n",
    "        accy_max = filt_ppg_freqs[np.argmax(accy_specs[:, i])]\n",
    "        accz_max = filt_ppg_freqs[np.argmax(accz_specs[:, i])]\n",
    "        sorted_ppg_specs = np.sort(filt_ppg_specs[:, i])[::-1]\n",
    "        \n",
    "        for f in range(10):\n",
    "            ppg_freq = filt_ppg_freqs[np.argwhere(filt_ppg_specs == sorted_ppg_specs[f])[0][0]]\n",
    "            if ppg_freq == 0:\n",
    "                continue\n",
    "            elif (\n",
    "                abs(ppg_freq - accx_max) <= distance_bps or\n",
    "                abs(ppg_freq - accy_max) <= distance_bps or\n",
    "                abs(ppg_freq - accz_max) <= distance_bps\n",
    "            ):\n",
    "                if f == 9:\n",
    "                    ppg_max_freqs.append(filt_ppg_freqs[np.argwhere(filt_ppg_specs == sorted_ppg_specs[0])[0][0]])\n",
    "                continue\n",
    "            else:\n",
    "                ppg_max_freqs.append(ppg_freq)\n",
    "                break\n",
    "\n",
    "    ecgdata = sp.io.loadmat(ref_fl)['BPM0']\n",
    "    print(f'filt_ppg_specs.shape: {filt_ppg_specs.shape}, ecgdata.shape: {ecgdata.shape}')\n",
    "\n",
    "    confidences = []\n",
    "    for i in range(min(filt_ppg_specs.shape[1], len(ecgdata))):\n",
    "        low_window = ppg_max_freqs[i] - BPS_SUM_WINDOW\n",
    "        high_window = ppg_max_freqs[i] + BPS_SUM_WINDOW\n",
    "        window = (filt_ppg_freqs >= low_window) & (filt_ppg_freqs <= high_window)\n",
    "        confidence = np.sum(filt_ppg_specs[:, i][window]) / np.sum(filt_ppg_specs[:, i])\n",
    "        error = abs(ppg_max_freqs[i] * 60 - ecgdata[i][0])\n",
    "        confidences.append((i, ppg_max_freqs[i] * 60, ecgdata[i][0], confidence, error))\n",
    "\n",
    "    confidence_df = pd.DataFrame(confidences, columns=['WindowNumber', 'Estimated_Pulse_Rate', 'Ref_BPM', 'Confidence', 'Error'])\n",
    "    errors = confidence_df['Error'].values\n",
    "    confidence = confidence_df['Confidence'].values\n",
    "\n",
    "    pulse_rates = np.array(ppg_max_freqs) * 60\n",
    "    infection_score, dehydration_score, arrhythmia_score = compute_pvr_score_components(pulse_rates)\n",
    "    print(f\"Infection Risk: {infection_score}, Dehydration Risk: {dehydration_score}, Arrhythmia Risk: {arrhythmia_score}\")\n",
    "\n",
    "\n",
    "    return errors, confidence, infection_score, dehydration_score, arrhythmia_score\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)  \n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence] \n",
    "    return np.mean(np.abs(best_estimates))  \n",
    "\n",
    "def Evaluate():\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    infection_scores = []\n",
    "    dehydration_scores = []\n",
    "    arrhythmia_scores = []\n",
    "\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        errors, confidence, infection, dehydration, arrhythmia = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        infection_scores.append(infection)\n",
    "        dehydration_scores.append(dehydration)\n",
    "        arrhythmia_scores.append(arrhythmia)\n",
    "\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "\n",
    "    df_summary = pd.DataFrame({\n",
    "        'File': [f.split(\"/\")[-1] for f in data_fls],\n",
    "        'Infection Risk': infection_scores,\n",
    "        'Dehydration Risk': dehydration_scores,\n",
    "        'Arrhythmia Risk': arrhythmia_scores\n",
    "    })\n",
    "\n",
    "    aggregate_error = AggregateErrorMetric(errs, confs)\n",
    "    print(f\"Aggregate Error (90% confident): {aggregate_error}\")\n",
    "    print(df_summary)\n",
    "\n",
    "    return errs, confs, df_summary\n",
    "\n",
    "\n",
    "# Run everything\n",
    "errors, confidence, summary = Evaluate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
